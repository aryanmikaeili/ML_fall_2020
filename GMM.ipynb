{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GMM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3j_3nUN2LC5"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aryanmikaeili/ML_fall_2020/blob/master/GMM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixRvk-18pXpg"
      },
      "source": [
        "#Introduction to Machine Learning - Sharif University of Technology, Fall 2020\n",
        "\n",
        "# Clustering (Kmeans , Gaussian Mixture Model) and EM algorithm - GMM (Part 2)\n",
        "\n",
        "\n",
        "__content creator:__ Aryan Mikaeili"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dm7MOmtepb9B"
      },
      "source": [
        "# Notebook Objective\n",
        "\n",
        "Up until now, you have been introduced to and worked with Supervised learning methods. In this method of learning the goal is clear; to produce desired output, given a set of inputs. However from now on, we want to explore another method of learning, namely Unsupervised learning. this method can be categorized as follows:\n",
        "1. Clustering \n",
        "2. Dimensionality reduction\n",
        "3. Data density estimation\n",
        "4. finding a hidden cause\n",
        "\n",
        "In the previous notebook, we went over the Kmeans algorithm. Although Kmeans is a simple and good way to approach clustering, It does not always work properly. For example let's try to cluster the data below using Kmeans.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt3ryHlzaCER",
        "cellView": "form"
      },
      "source": [
        "#@title Imports: Please run this cell to set notebooks environment\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "import random\n",
        "\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAkMpjUjaQ5k",
        "cellView": "form"
      },
      "source": [
        "#@title Helper functions: please run this cell to set notebooks environment\n",
        "\n",
        "def find_distance(x, y):\n",
        "\n",
        "  \"\"\" please do not edit this function \"\"\"\n",
        "\n",
        "  ##################################Inputs##################################\n",
        "  # x : (N1 * f) ndarray\n",
        "  # y : (N2 * f) ndarray\n",
        "  ##########################################################################\n",
        "\n",
        "  ##################################Outputs##################################\n",
        "  #o:  (N1 * N2) ndarray , o[i, j] is the distance of x[i] and y[j] squared\n",
        "  ##########################################################################\n",
        "\n",
        "  k = y.shape[0]\n",
        "  data_size = x.shape[0]\n",
        "\n",
        "  x_norm = np.repeat(np.expand_dims(np.linalg.norm(x, axis = 1) ** 2, 1), k, axis = 1)\n",
        "  y_norm = np.repeat(np.expand_dims(np.linalg.norm(y, axis = 1) ** 2, 0), data_size, axis = 0)\n",
        "  x_y_inner = np.matmul(x, y.T)\n",
        "\n",
        "  o = x_norm + y_norm - 2 * x_y_inner\n",
        "  return o\n",
        "\n",
        "class Kmeans:\n",
        "\n",
        "  def __init__(self, x, k, max_iters = 1000):\n",
        "    self.x = x\n",
        "    self.data_size = x.shape[0]\n",
        "    self.feature_size = x.shape[1]\n",
        "    self.k = k\n",
        "    self.max_iters = max_iters\n",
        "\n",
        "    self.centers = np.zeros((k, x.shape[1]))\n",
        "  \n",
        "    self.predictions = np.zeros(self.data_size)\n",
        "\n",
        "\n",
        "\n",
        "  def Kmeans_plus_plus(self):\n",
        "    centers = np.zeros((self.k, self.feature_size))\n",
        "  \n",
        "    centers[0] = random.choices(self.x)[0]\n",
        "\n",
        "    for i in range(1, self.k):\n",
        "      current_centers = centers[: i]\n",
        "      data_center_dists = find_distance(self.x, current_centers)\n",
        "\n",
        "      min_dists = data_center_dists.min(axis = 1)\n",
        "      centers[i] = random.choices(self.x, weights = min_dists)[0]\n",
        "    return centers\n",
        "\n",
        "  def random_init(self):\n",
        "      mean = np.mean(self.x, axis = 0)\n",
        "      std = np.std(self.x, axis = 0)\n",
        "\n",
        "      centers = np.random.randn(self.k, self.feature_size) * mean + std\n",
        "\n",
        "      return centers\n",
        "\n",
        "\n",
        "  def predict(self, init_mode = 'random'):\n",
        "\n",
        "    if init_mode == 'random':\n",
        "      self.centers = self.random_init()\n",
        "    elif init_mode == 'kmeans++':\n",
        "      self.centers = self.Kmeans_plus_plus()\n",
        "    for i in range(self.max_iters):\n",
        "      data_center_dists = find_distance(self.x, self.centers)\n",
        "      self.predictions = data_center_dists.argmin(axis = 1)\n",
        "\n",
        "      prev_centers = np.copy(self.centers)\n",
        "\n",
        "      for j in range(self.k):\n",
        "        cluster_j = self.x[self.predictions == j] \n",
        "        if len(cluster_j) > 0:\n",
        "          self.centers[j] = cluster_j.mean(axis = 0)\n",
        "      \n",
        "      if (prev_centers == self.centers).all():\n",
        "        return\n",
        "\n",
        "\n",
        "  def calc_distortion(self):\n",
        "    distortion = 0\n",
        "\n",
        "    for i in range(self.k):\n",
        "      cluster_x = self.x[self.predictions == i]\n",
        "      cluster_center = self.centers[i]\n",
        "      if len(cluster_x) > 0:\n",
        "        cluster_distances = find_distance(cluster_x, np.expand_dims(cluster_center, 0))\n",
        "\n",
        "        distortion += cluster_distances.sum()\n",
        "    \n",
        "    return distortion / self.data_size\n",
        "\n",
        "\n",
        "from matplotlib.patches import Ellipse\n",
        "import matplotlib.transforms as transforms\n",
        "\n",
        "def confidence_ellipse(cov, mean_x, mean_y, ax, n_std=3.0, facecolor='none', **kwargs):\n",
        "\n",
        "    pearson = cov[0, 1]/np.sqrt(cov[0, 0] * cov[1, 1])\n",
        "    # Using a special case to obtain the eigenvalues of this\n",
        "    # two-dimensionl dataset.\n",
        "    ell_radius_x = np.sqrt(1 + pearson)\n",
        "    ell_radius_y = np.sqrt(1 - pearson)\n",
        "    ellipse = Ellipse((0, 0),\n",
        "        width=ell_radius_x * 2,\n",
        "        height=ell_radius_y * 2,\n",
        "        facecolor=facecolor,\n",
        "        **kwargs)\n",
        "\n",
        "    # Calculating the stdandard deviation of x from\n",
        "    # the squareroot of the variance and multiplying\n",
        "    # with the given number of standard deviations.\n",
        "    scale_x = np.sqrt(cov[0, 0]) * n_std\n",
        "\n",
        "    # calculating the stdandard deviation of y ...\n",
        "    scale_y = np.sqrt(cov[1, 1]) * n_std\n",
        "\n",
        "    transf = transforms.Affine2D() \\\n",
        "        .rotate_deg(45) \\\n",
        "        .scale(scale_x, scale_y) \\\n",
        "        .translate(mean_x, mean_y)\n",
        "\n",
        "    ellipse.set_transform(transf + ax.transData)\n",
        "    return ax.add_patch(ellipse)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6faAvVbZ4Rj",
        "cellView": "form"
      },
      "source": [
        "#@title Data Generation\n",
        "\n",
        "cluster1 = np.random.multivariate_normal([0, 0], [[1, 0],[0, 2]], 100)\n",
        "cluster2 = np.random.multivariate_normal([2,2], [[1, 0],[0, 10]], 100)\n",
        "\n",
        "data = np.row_stack([cluster1, cluster2])\n",
        "\n",
        "plt.scatter(cluster1[:, 0], cluster1[:, 1])\n",
        "plt.scatter(cluster2[:, 0], cluster2[:, 1])\n",
        "\n",
        "plt.legend(['cluster 1', 'cluster 2'])\n",
        "plt.title('original data: as you can see the two clusters are extremely close')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FK9DIpgpaftb"
      },
      "source": [
        "model = Kmeans(data, 2)\n",
        "\n",
        "\n",
        "model.predict('kmeans++')\n",
        "\n",
        "plt.scatter(data[model.predictions == 0, 0], data[model.predictions == 0, 1])\n",
        "plt.scatter(data[model.predictions == 1, 0], data[model.predictions == 1, 1])\n",
        "\n",
        "plt.legend(['cluster 1', 'cluster 2'])\n",
        "plt.title('Kmeans cannot provide a reasonable clustering in this case')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4UUWLrPZ3GR"
      },
      "source": [
        "As a result we should come up with smarter ways to do clustering.\n",
        "\n",
        "In this Notebook, we Introduce Gaussian Mixture Models (GMMs); In which we assume that the whole data comes from a mixture of __k__ gaussian distributions and our goal is to find a way to estimate the parameters of these distributions. In summary, in this notebook we will:\n",
        "1. Review GMMs and formulize the problem\n",
        "2. implement GMM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrTQddmIiB7p"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "As you recall, in the GMM model, we try to fit __k__ gaussian distributions to the set of datapoints we have. and oure goal is to estimate the parameters, i.e. the mean and the covariance matrix of these gaussian distribution.\n",
        "\n",
        "Let's assume we have a distribution __z__ which has a 1-of-K representation; meaning that it is a k dimensional binary vector, with ony a 1 and the rest zero and the 1 indicates from which gaussian distribution a datapoint comes from. That means that $z$ is a distribution with k states.\n",
        "\n",
        "now we can write:\n",
        "\n",
        "$$p(z_k = 1) = \\pi_k$$\n",
        "\n",
        "$\\pi_k$ being a variable satisfying $\\sum_k{\\pi_k} = 1$.\n",
        "\n",
        "So we have:\n",
        "$$ p(z) = \\prod_k{\\pi_k^{z_k}}$$\n",
        "\n",
        "we also have:\n",
        "\n",
        "$$ p(x|z) = \\sum_k{\\mathbb{N}(x | \\mu_k, \\Sigma_k) ^ z_k} $$\n",
        "\n",
        "As a result if we want to write the marginal distribution of $p(x)$, we have to write:\n",
        "\n",
        "$$p(x) = \\sum_k{p(x|z)p(z)} =  \\sum_k{\\pi_k \\mathbb{N}(x | \\mu_k, \\Sigma_k)}$$\n",
        "\n",
        "To estimate the parameters of the Gaussians, we will first write the maximum log-likelihood of the data.\n",
        "\n",
        "$$\\log p(X|z, \\theta) = \\sum_n{\\log{\\sum_k{\\pi_k \\mathbb{N}(x | \\mu_k, \\Sigma_k)}}}$$\n",
        "\n",
        "Now had it been the case that we had a single gaussian, we would have simply calculated the derivative of this function with respect to the parmaters ($\\theta$) and calculated the optimum parameters. But in this case we cannot do that because of singularities. Assume that one of the gaussians collapses into a single datapoint. i.e. the mean of the gaussian becomes equal to the datapoint and the standard deviations becomes zero. So the result of $\\mathbb{N}(x | \\mu_k, \\Sigma_k)$ would be  infinity for this particular datapoint and as a result the log-likelihood of the data will tend to infinity. As mentioned before if we had a single gaussian distribution and this collapse happened, the probabilty of the other datapoints would have been equal to zero. So the probability of the whole data would have been zero.\n",
        "\n",
        "Because of the problem mentioned above, to estimate the parameters of a GMM, we cannot use log-likelihood estimation. Instead, we use the EM algorithm. which is an algorithm that by maximizing another function (called variational lower bound) ensures that the log-likelihood function decreases too.\n",
        "\n",
        "The details of the EM algorithm in general would be the topic of discussion for the next notebook. however, this algorithm, comes very naturally when solving the GMM problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YL_yLizUD3Qr"
      },
      "source": [
        "# EM algorithm for GMM\n",
        "\n",
        "Before we dive into the EM algorithm, we first calculate $p(z_k = 1|x_n, \\theta)$ and call it $\\gamma_n(z_k)$. Because this would come in handy in the steps of the EM algorithm.\n",
        "\n",
        "Using the Bayes rule we have:\n",
        "\n",
        "$$p(z_k = 1|x_n, theta) =  \\frac{p(x_n|z_k = 1, \\theta) p(z_k = 1|\\theta)}{p(x_n|\\theta)} =  \\frac{p(x_n|z_k = 1, \\theta) p(z_k = 1|\\theta)}{\\sum_j{p(x_n|z_j = 1)p(z_j = 1)}} = \\frac{\\pi_k \\mathbb{N}(x | \\mu_k, \\Sigma_k)}{\\sum_j{\\pi_j \\mathbb{N}(x | \\mu_j, \\Sigma_j)}}$$\n",
        "\n",
        "Now that we have caluclated everything that we need, we will solve the GMM problem.\n",
        "We first need to estimate the $\\mu_k$ parameters. For that, we take the derivative of the log_likelihood function with respect to $\\mu_k$ and set it to zero. So we will have:\n",
        "\n",
        "$$0 = \\sum_n\\;\\underbrace{\\frac{\\pi_k \\mathbb{N}(x | \\mu_k, \\Sigma_k)}{\\sum_j{\\pi_j \\mathbb{N}(x | \\mu_j, \\Sigma_j)}}}_{\\gamma_n(z_k)}\\Sigma_k(x_n - \\mu_k) \\Rightarrow \\mu_k = \\frac{1}{N_k}\\sum_n{\\gamma_n(z_k)x_n} $$\n",
        "\n",
        "In the final result, $N_k = \\sum_n{\\gamma_n(z_k)}$.\n",
        "\n",
        "As you can understand, the phrase which is equal to $\\gamma_n(z_k)$ has $\\mu_k$ in it. but this simplifying assumption that this phrase is constant and equal to $\\gamma_n(z_k)$ is how the EM algorithm approaches this problem.\n",
        "\n",
        "For the other parameters also, we can simply derive that:\n",
        "\n",
        "$$\\Sigma_k = \\frac{1}{N_k}\\sum_n{\\gamma_n(z_k)(x_n - \\mu_k)(x_n - \\mu_k)^T}$$\n",
        "$$\\pi_k = \\frac{N_k}{N}$$\n",
        "\n",
        "in which $N$ is the total number of datapoints.\n",
        "\n",
        "In summary the EM algorithm has two steps:\n",
        "1. the Expectation step (E step): calculate the $\\gamma_n(z_k)$s.\n",
        "2. the Maximization step (M step): maximize the log-likelihood function using the equations above.\n",
        "\n",
        "in the next notebook we will in detail see that why this algorithm works and extend it for further use in other applications.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOnbyj-mZYnj"
      },
      "source": [
        "# Implementation\n",
        "\n",
        "Now using the equations provided above, we will try to implement the Gaussian Mixture Model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_C46khK-x47G"
      },
      "source": [
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "class GMM:\n",
        "  def __init__(self, x, k, max_iter = 1000, threshold = 1e-4):\n",
        "    ##################################Inputs##################################\n",
        "    # x: the data we are clustering, a (N * k) ndarray\n",
        "    # k: number of clusters\n",
        "    # max_iter: the maximum number of iteration before aborting the algorithm\n",
        "    # threshold: abort algorithm if negative log-likelihoods change is below this threshold\n",
        "    ##########################################################################\n",
        "    self.x = x\n",
        "    #number of datapoints    \n",
        "    self.data_size = self.x.shape[0]\n",
        "\n",
        "    #dimension of the space\n",
        "    self.feature_size = self.x.shape[1]\n",
        "    \n",
        "    self.k = k\n",
        "\n",
        "    self.max_iter = max_iter\n",
        "\n",
        "    self.threshold = threshold\n",
        "\n",
        "    #the final clustering result\n",
        "    self.predictions = np.zeros(self.data_size)\n",
        "\n",
        "    #parameters of the model\n",
        "    self.Pi = None\n",
        "    self.Mu = None\n",
        "    self.Sigma = None\n",
        "\n",
        "    #gamma or p(z|x)\n",
        "    self.gammas = np.zeros((self.data_size, self.k))\n",
        "  \n",
        "  def init_params(self):\n",
        "\n",
        "  ###############################################################################\n",
        "    ## Insert your code here to:\n",
        "    #   * Initalizes Parameters of Model. i.e. Mu, Sigma and Pi.\n",
        "    #   * Mu: randomly chosen among datapoints\n",
        "    #   * Sigma: Identity Matrix\n",
        "    #.  * Pi: Uniform. probability of data coming from each cluster is equal\n",
        "\n",
        "    raise NotImplementedError(\"Please complete This part\")\n",
        "    ###############################################################################\n",
        "    \n",
        "    self.Pi = ...\n",
        "\n",
        "    self.Mu = ...\n",
        "\n",
        "    self.Sigma =  ...\n",
        "\n",
        "  \n",
        "  def calc_gammas(self):\n",
        "\n",
        "  ###############################################################################\n",
        "    ## Insert your code here to:\n",
        "    #   * Calculate gammas\n",
        "    raise NotImplementedError(\"Please complete This part\")\n",
        "    ###############################################################################\n",
        "\n",
        "  def update_mu(self, N_k):\n",
        "    ##################################Inputs##################################\n",
        "    # N_k: sum of gamma(n, k) over n\n",
        "    ##########################################################################\n",
        "\n",
        "    ###############################################################################\n",
        "    ## Insert your code here to:\n",
        "    #   * update Mu\n",
        "    raise NotImplementedError(\"Please complete This part\")\n",
        "    ###############################################################################\n",
        "\n",
        "\n",
        "  def update_sigma(self, N_k):\n",
        "    ##################################Inputs##################################\n",
        "    # N_k: sum of gamma(n, k) over n\n",
        "    ##########################################################################\n",
        "\n",
        "    ###############################################################################\n",
        "    ## Insert your code here to:\n",
        "    #   * update Sigma\n",
        "    raise NotImplementedError(\"Please complete This part\")\n",
        "    ###############################################################################\n",
        "\n",
        "  def update_pi(self, N_k):\n",
        "    ##################################Inputs##################################\n",
        "    # N_k: sum of gamma(n, k) over n\n",
        "    ##########################################################################\n",
        "\n",
        "    ###############################################################################\n",
        "    ## Insert your code here to:\n",
        "    #   * update Pi\n",
        "    raise NotImplementedError(\"Please complete This part\")\n",
        "    ###############################################################################\n",
        "\n",
        "\n",
        "  #the following 3 functions are for calculating log-likelihood\n",
        "  def calc_nmat(self):\n",
        "     diff = np.reshape(self.x, (self.data_size, 1, self.feature_size)) - np.reshape(self.Mu, (1, self.k, self.feature_size))\n",
        "     L = np.linalg.inv(self.Sigma)\n",
        "     exponent = np.einsum(\"nkj,nkj->nk\", np.einsum(\"nki,kij->nkj\", diff, L), diff)\n",
        "     Nmat = np.exp(-0.5 * exponent) / np.sqrt(np.linalg.det(self.Sigma))   / ((2 * np.pi) ** (self.feature_size/2))\n",
        "\n",
        "     return Nmat\n",
        "\n",
        "  \n",
        "  def calc_probability(self):\n",
        "    nmat = self.calc_nmat()\n",
        "\n",
        "    probs = nmat @ self.Pi\n",
        "\n",
        "    return probs\n",
        "\n",
        "  def calc_log_likelihood(self):\n",
        "    probs = self.calc_probability()\n",
        "\n",
        "    log_prob = np.log(probs)\n",
        "\n",
        "    nll = -np.sum(log_prob)\n",
        "\n",
        "    return nll\n",
        "\n",
        "  def predict(self):\n",
        "\n",
        "    ###############################################################################\n",
        "    ## Insert your code here to:\n",
        "    #.  * Initialize Parameters\n",
        "    #   * Perform the E step\n",
        "    #   * Perform the M step\n",
        "    #   * determine cluster of each datapoint\n",
        "  \n",
        "    raise NotImplementedError(\"Please complete This part\")\n",
        "    ###############################################################################\n",
        "\n",
        "    #Initialize Parameters\n",
        "\n",
        "    ...\n",
        "\n",
        "    #negative log_likelihood initialization\n",
        "\n",
        "    nll = np.inf\n",
        "\n",
        "    for i in range(self.max_iter):\n",
        "      #E step\n",
        "      ...\n",
        "\n",
        "      #M_step\n",
        "\n",
        "      ...\n",
        "\n",
        "      # algorithm abortion criteria\n",
        "      old_nll = nll\n",
        "\n",
        "      nll = self.calc_log_likelihood()\n",
        "\n",
        "      if old_nll - nll < self.threshold:\n",
        "        break     \n",
        "   \n",
        "\n",
        "      \n",
        "\n",
        "    #determine each datapoints cluster\n",
        "    ...\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUE5TIRecxLn",
        "cellView": "form"
      },
      "source": [
        "#@title click to view solution or run the cell\n",
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "class GMM:\n",
        "  def __init__(self, x, k, max_iter = 1000, threshold = 1e-4):\n",
        "    ##################################Inputs##################################\n",
        "    # x: the data we are clustering, a (N * k) ndarray\n",
        "    # k: number of clusters\n",
        "    # max_iter: the maximum number of iteration before aborting the algorithm\n",
        "    # threshold: abort algorithm if negative log-likelihoods change is below this threshold\n",
        "    ##########################################################################\n",
        "    self.x = x\n",
        "    #number of datapoints    \n",
        "    self.data_size = self.x.shape[0]\n",
        "\n",
        "    #dimension of the space\n",
        "    self.feature_size = self.x.shape[1]\n",
        "    \n",
        "    self.k = k\n",
        "\n",
        "    self.max_iter = max_iter\n",
        "\n",
        "    self.threshold = threshold\n",
        "\n",
        "    #the final clustering result\n",
        "    self.predictions = np.zeros(self.data_size)\n",
        "\n",
        "    #parameters of the model\n",
        "    self.Pi = None\n",
        "    self.Mu = None\n",
        "    self.Sigma = None\n",
        "\n",
        "    #gamma or p(z|x)\n",
        "    self.gammas = np.zeros((self.data_size, self.k))\n",
        "  \n",
        "  def init_params(self):\n",
        "    self.Pi = np.ones(self.k) / self.k\n",
        "\n",
        "    self.Mu = self.x[np.random.choice(self.data_size, self.k, replace = False)]\n",
        "\n",
        "    self.Sigma =  np.repeat(np.expand_dims(np.eye(N = self.feature_size), 0), self.k , axis = 0)\n",
        "\n",
        "  def calc_nmat(self):\n",
        "     diff = np.reshape(self.x, (self.data_size, 1, self.feature_size)) - np.reshape(self.Mu, (1, self.k, self.feature_size))\n",
        "     L = np.linalg.inv(self.Sigma)\n",
        "     exponent = np.einsum(\"nkj,nkj->nk\", np.einsum(\"nki,kij->nkj\", diff, L), diff)\n",
        "     Nmat = np.exp(-0.5 * exponent) / np.sqrt(np.linalg.det(self.Sigma))   / ((2 * np.pi) ** (self.feature_size/2))\n",
        "\n",
        "     return Nmat\n",
        "  \n",
        "  def calc_probability(self):\n",
        "    nmat = self.calc_nmat()\n",
        "\n",
        "    probs = nmat @ self.Pi\n",
        "\n",
        "    return probs\n",
        "\n",
        "  def calc_log_likelihood(self):\n",
        "    probs = self.calc_probability()\n",
        "\n",
        "    log_prob = np.log(probs)\n",
        "\n",
        "    nll = -np.sum(log_prob)\n",
        "\n",
        "    return nll\n",
        "\n",
        "\n",
        "  def calc_gammas(self):\n",
        "\n",
        "    for i in range(self.data_size):\n",
        "      for j in range(self.k):\n",
        "          self.gammas[i, j] = self.Pi[j] * multivariate_normal.pdf(self.x[i], self.Mu[j], self.Sigma[j])\n",
        "    \n",
        "      self.gammas[i] = self.gammas[i] / self.gammas[i].sum()\n",
        "\n",
        "  def update_mu(self, N_k):\n",
        "    self.Mu = (1 / N_k).reshape(-1, 1) * np.matmul(self.gammas.T, self.x) \n",
        "\n",
        "  def update_sigma(self, N_k):\n",
        "    diff = np.reshape(self.x, (self.data_size, 1, self.feature_size)) - np.reshape(self.Mu, (1, self.k, self.feature_size))\n",
        "\n",
        "    self.Sigma = np.einsum('nki, nkj->kij', np.einsum('nk, nki -> nki', self.gammas, diff), diff) / np.reshape(N_k, (self.k, 1, 1))\n",
        "  def update_pi(self, N_k):\n",
        "    self.Pi = N_k / self.data_size\n",
        "\n",
        "  def predict(self):\n",
        "    self.init_params()\n",
        "\n",
        "    nll = np.inf\n",
        "    for i in range(self.max_iter):\n",
        "      #E step\n",
        "      self.calc_gammas()\n",
        "\n",
        "      #M_step\n",
        "\n",
        "      N_k = np.sum(self.gammas, axis = 0)\n",
        "\n",
        "      self.update_mu(N_k)\n",
        "      self.update_sigma(N_k)\n",
        "      self.update_pi(N_k)\n",
        "\n",
        "      old_nll = nll\n",
        "\n",
        "      nll = self.calc_log_likelihood()\n",
        "      if old_nll - nll < self.threshold:\n",
        "        break     \n",
        "\n",
        "      \n",
        "\n",
        "    self.predictions = np.argmax(self.gammas, axis = 1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MZ64ceLLlN1"
      },
      "source": [
        "model = GMM(data, 2, max_iter = 100)\n",
        "model.predict()\n",
        "\n",
        "\n",
        "fig, axs = plt.subplots(1, 3, figsize = (20, 5))\n",
        "\n",
        "axs[0].scatter(data[model.predictions == 0, 0], data[model.predictions == 0, 1])\n",
        "axs[0].scatter(data[model.predictions == 1, 0], data[model.predictions == 1, 1])\n",
        "\n",
        "axs[0].legend(['cluster 1', 'cluster 2'])\n",
        "\n",
        "axs[0].set_title('GMM')\n",
        "\n",
        "\n",
        "model = Kmeans(data, 2)\n",
        "model.predict('kmeans++')\n",
        "axs[1].scatter(data[model.predictions == 0, 0], data[model.predictions == 0, 1])\n",
        "axs[1].scatter(data[model.predictions == 1, 0], data[model.predictions == 1, 1])\n",
        "\n",
        "axs[1].legend(['cluster 1', 'cluster 2'])\n",
        "\n",
        "axs[1].set_title('Kmeans')\n",
        "\n",
        "axs[2].scatter(cluster1[:, 0], cluster1[:, 1])\n",
        "axs[2].scatter(cluster2[:, 0], cluster2[:, 1])\n",
        "\n",
        "axs[2].legend(['cluster 1', 'cluster 2'])\n",
        "\n",
        "axs[2].set_title('Original clusters')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2DzDe90lEBR"
      },
      "source": [
        "Now let's see how the gaussians that our model predicted actually look like in the plot. The ellipses plotted are the confidence ellipses of the data. with center on the mean of the corresponing gaussian and radius proportional to variance of data in x and y direction.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCx4EH57Q4PU"
      },
      "source": [
        "model = GMM(data, 2, max_iter = 100)\n",
        "model.predict()\n",
        "\n",
        "fig, axs = plt.subplots(figsize = (5,  5))\n",
        "\n",
        "axs.scatter(data[model.predictions == 0, 0], data[model.predictions == 0, 1], s = 5, c = 'blue')\n",
        "axs.scatter(model.Mu[0][0], model.Mu[0][1], s = 500, marker = 'o', c = 'blue')\n",
        "confidence_ellipse(model.Sigma[0], model.Mu[0][0], model.Mu[0][1], axs, edgecolor = 'blue')\n",
        "\n",
        "axs.scatter(data[model.predictions == 1, 0], data[model.predictions == 1, 1], s = 5, c = 'red')\n",
        "axs.scatter(model.Mu[1][0], model.Mu[1][1], s = 500, marker = 'o', c = 'red')\n",
        "confidence_ellipse(model.Sigma[1], model.Mu[1][0], model.Mu[1][1], axs, edgecolor = 'red')\n",
        "\n",
        "axs.legend(['cluster 1', 'cluster 2'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}