{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EM.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0KJZgD-nayY"
      },
      "source": [
        "#Introduction to Machine Learning - Sharif University of Technology, Fall 2020\n",
        "\n",
        "# Clustering (Kmeans , Gaussian Mixture Model) and EM algorithm - Part 3 (the EM algorithm)\n",
        "\n",
        "__content creator:__ Aryan Mikaeili\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnteVOxdnheb"
      },
      "source": [
        "# Notebook Objective\n",
        "\n",
        "Up until now, you have been introduced to and worked with Supervised learning methods. In this method of learning the goal is clear; to produce desired output, given a set of inputs. However from now on, we want to explore another method of learning, namely Unsupervised learning. this method can be categorized as follows:\n",
        "1. Clustering \n",
        "2. Dimensionality reduction\n",
        "3. Data density estimation\n",
        "4. finding a hidden cause\n",
        "\n",
        "In the previous notebook we explored Gaussian Mixture models as a way to cluster data; and we saw how solving the GMM model with normal log-likelihood optimization lead to problems and we approached it with the EM algorithm which basically in the context of GMM was some simplifying assumptions on the target function we were trying to optimize.\n",
        "\n",
        "In this notebook, we will conclude the subject of clustering with the general form of EM algorithm and we will see how we can solve other problems with latent variables using EM algorithm\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVqcZtpsqwZh"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Assume we have a probabilistic problem with latent variables and data which we call $Z$ and $X$ respectively and the joint probability distribution of $X$ and $Z$ is governed by a set of parameters, called $\\theta$.\n",
        "\n",
        "In this case we can write the likelihood of the data as follows:\n",
        "\n",
        "$$\n",
        "P(X|\\theta) = \\sum_z{P(X,Z|\\theta)}\n",
        "$$\n",
        "\n",
        "So the log-likelihood of the data will become:\n",
        "\n",
        "$$\\log{P(X|\\theta)} = \\log{\\sum_z{P(X,Z|\\theta)}}$$\n",
        "\n",
        "As we saw in the previous notebook, the fact that the logarithm function isn't directly applied to the probability function, makes optimizing this log-likelihood problem very dificult. As a result we try to come of with smarter ways to solve the problem.\n",
        "\n",
        "First let's write the likelihood function as sum of the two following terms:\n",
        "1. $$KL(q||p) = -\\sum_z{q(z) \\ln{\\frac{P(Z|X, \\theta)}{q(z)}}}$$\n",
        "2.$$ L(q, p) = \\sum_z{q(z) \\ln{\\frac{P(X,Z|\\theta)}{q(z)}}}$$\n",
        "\n",
        "In which $q(z)$ is an arbitrary probability distribution function over z.\n",
        "\n",
        "You can easily verify that:\n",
        "\n",
        "$$\\ln P(X|\\theta) = L(q, \\theta) + KL(q||p)$$\n",
        "\n",
        "As you know, the KL divergence term, is always positive. As a result we have:\n",
        "$$ L(q, \\theta) < P(X|\\theta) $$\n",
        "\n",
        "As you can see, the $L(q, \\theta)$ function is a lower bound for $P(X|\\theta)$. so we call it __Variational lower bound__.\n",
        "\n",
        "The EM algorihtm is a two stage algorithm which in which in the first step, we fix $\\theta$ and optimize $L(q, \\theta)$ so it gets close to the likelihood function. you can see that this happens when the KL divergence term is equal to zero. i.e. when $q(z)$ is equal to $P(Z|X, \\theta)$.\n",
        "\n",
        "This is equivalent to the E step of the EM algorithm.\n",
        "\n",
        "In the next step we fix $q(z)$ and maximize $L(q, \\theta)$ with respect to $\\theta$. i.e. we solve the following optimization problem.\n",
        "\n",
        "$$\\min_\\theta{L(q, \\theta)} = \\min_\\theta{\\sum_z{q(z) \\ln{\\frac{P(X,Z|\\theta)}{q(z)}}}} = \\min_\\theta{\\sum_z{P(Z|X, \\theta)\\ln{P(X,Z|\\theta)}} - \\sum_z{q(z)\\ln{q(z)}}}$$\n",
        "\n",
        "The second term in the above equation is purely a function of $q$ which we considered constant in this step. So the problem becomes:\n",
        "\n",
        "$$\\min_\\theta{\\sum_z{P(Z|X, \\theta)\\ln{P(X,Z|\\theta)}}} = E_{P(Z|X, \\theta)}[\\ln{P(X,Z|\\theta)}]$$\n",
        "\n",
        "In this equation, the summation appears outside of the logarithm. as a result it generaly makes the problem more approachable.\n",
        "\n",
        "Also, know you can see the idea behind the terminology of the EM algorithm.\n",
        "in the Expectation step, we calculate the expectation of $P(X,Z|\\theta)$ with respect to $P(Z|X,\\theta)$ and in the Maximization step, we maximize this expectation with respect to $\\theta$.\n",
        "\n",
        "Another important point is how this algorithm maximizes the log-likelihood function. In each step, the lower bound ($L(q, \\theta_{old}$) is set to equal the log likelihod function. then by maximizing this lower bound with respect to $\\theta$, we can easily obtain that $P(X|\\theta_{new}) > L(q, \\theta_{new}) > L(q, \\theta_{old}) = P(X|\\theta_{old})$"
      ]
    }
  ]
}