{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kmeans.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgjs15CmcJrT"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aryanmikaeili/ML_fall_2020/blob/master/kmeans.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJk0ohd074EV"
      },
      "source": [
        "#Introduction to Machine Learning - Sharif University of Technology, Fall 2020\n",
        "\n",
        "# Clustering (Kmeans , Gaussian Mixture Model) and EM algorithm - Part 1 (Kmeans)\n",
        "\n",
        "__content creator:__ Aryan Mikaeili\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9o-7HLME8edn"
      },
      "source": [
        "# Notebook Objective\n",
        "\n",
        "Up until now, you have been introduced to and worked with Supervised learning methods. In this method of learning the goal is clear; to produce desired output, given a set of inputs. However from now on, we want to explore another method of learning, namely Unsupervised learning. this method can be categorized as follows:\n",
        "1. Clustering \n",
        "2. Dimensionality reduction\n",
        "3. Data density estimation\n",
        "4. finding a hidden cause\n",
        "\n",
        "In this Notebook, we want to introduce and implement Kmeans algorithm. A simple and effective algorithm for clustering data.\n",
        "\n",
        "In this notebook, we will:\n",
        "1. review the Kmeans algorithm\n",
        "2. implement the Kmeans algorithm\n",
        "3. learn how to set k parameter\n",
        "4. use Kmeans for image segmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HL6LTC4f0Tb1"
      },
      "source": [
        "# Review\n",
        "\n",
        "As you recall, In Kmeans our goal is to divide a set of datapoints into __k__ clusters. To do so, we define the below cost function and try to minimize it.\n",
        "\n",
        "$$ J(X, M) = \\sum_k{\\sum_n{r^{k}_{n}||m_k - x_n|| ^ 2}}$$\n",
        "$$s.t.  \\qquad r_n^k = \\{0, 1\\} \\qquad  \\sum_k r_n^k = 1$$\n",
        "\n",
        "In which $x_n$s are the datapoints, $m_k$s are cluster centers and $r_n^k$ is a binary variable indicating whether datapoint $x_n$ is in cluster $k$ or not. In short terms, the algorithm, wants to place each cluster, and assign each point to a cluster, in a way that the sum of distances of every datapoint to their corresponding cluster centers is minimal.\n",
        "\n",
        "This cost function, cannot be optimized in polynomial time, so we use an algorithm which consists of two steps:\n",
        "1. by fixing the centers, we assign each datapoint to the closest center.\n",
        "2. by fixing the datapoint assignments, we determine the center of each cluster by averaging the datapoints assigned to each cluster.\n",
        "\n",
        "This method does not guarantee that the cost function reaches its global minimum, However, it is easy to show that in each step the cost function decreases (if it has not yet reached a local minima).\n",
        "\n",
        "In the next part, we will implement the Kmeans algorithm an try to address some of the technical difficulties of the algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUDxS-0W8IDH"
      },
      "source": [
        "# Implementation\n",
        "\n",
        "Please execute the cells below to initialize the notebook's environment and generate the data we are going to work with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBpZsER4Y-Ts",
        "cellView": "form"
      },
      "source": [
        "#@title Imports\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "import random\n",
        "\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")\n",
        "%matplotlib inline\n",
        "%config InlineBackend.figure_format = 'retina'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFhO5yaX8al_",
        "cellView": "form"
      },
      "source": [
        "#@title Data generation\n",
        "data = make_blobs(n_samples = [100, 100, 100], centers=[[0, 0], [0, 10], [10, 0]])[0]\n",
        "\n",
        "plt.scatter(data[:, 0], data[:, 1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpnBf0DF9BSn"
      },
      "source": [
        "Now we want to implement the Kmeans algorithm. In order to do so we first discuss the problem of initializing the centers.\n",
        "\n",
        "As you know, the Kmeans algorithm can easily get stuck in local minima and the final result of the algorithm, is highly dependent on the initialization of the cluster centers. As a result, we should be smart about initializing the cluster centers. There are multiple ways that we can achieve this objective:\n",
        "\n",
        "1. randomly choose some of the datapoints\n",
        "2. randomly generate the centers, from a gaussian distribution with mean and standard deviation equal to the mean and standard deviation of the dat.\n",
        "3. Kmeans++ algorithm: This is a more sophisticated algorithm than the ones mentioned above. This algorithm, starts with first choosing a datapoint randomly. \n",
        "Then at the next $k - 1$ steps, in each step, we find the distance of each datapoint to the currently chosen centers and choose the next cluster center randomly from the datapoints with a probability proportional to each datapoints distance to their closest currently chosen center.\n",
        "\n",
        "\n",
        "In our implementation of Kmeans, we use the Kmeans++ algorithm and random gaussian initialization.\n",
        "\n",
        "The function below, is a function which calculates the pairwise distance of a set of points to another set of points. This will be useful both in implementing the Kmeans++ algorithm and the Kmeans algorithm itself."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cscuBfblx1ck"
      },
      "source": [
        "def find_distance(x, y):\n",
        "\n",
        "  \"\"\" please do not edit this function \"\"\"\n",
        "\n",
        "  ##################################Inputs##################################\n",
        "  # x : (N1 * f) ndarray\n",
        "  # y : (N2 * f) ndarray\n",
        "  ##########################################################################\n",
        "\n",
        "  ##################################Outputs##################################\n",
        "  #o:  (N1 * N2) ndarray , o[i, j] is the distance of x[i] and y[j] squared\n",
        "  ##########################################################################\n",
        "\n",
        "  k = y.shape[0]\n",
        "  data_size = x.shape[0]\n",
        "\n",
        "  x_norm = np.repeat(np.expand_dims(np.linalg.norm(x, axis = 1) ** 2, 1), k, axis = 1)\n",
        "  y_norm = np.repeat(np.expand_dims(np.linalg.norm(y, axis = 1) ** 2, 0), data_size, axis = 0)\n",
        "  x_y_inner = np.matmul(x, y.T)\n",
        "\n",
        "  o = x_norm + y_norm - 2 * x_y_inner\n",
        "  return o"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnYAskWHCcbR"
      },
      "source": [
        "class Kmeans:\n",
        "  def __init__(self, x, k, max_iters = 1000):\n",
        "    ##################################Inputs##################################\n",
        "    # x: the data we are clustering, a (N * k) ndarray\n",
        "    # k: number of clusters\n",
        "    # max_iter: the maximum number of iteration before aborting the algorithm\n",
        "    ##########################################################################\n",
        "    self.x = x\n",
        "\n",
        "    #number of datapoints\n",
        "    self.data_size = x.shape[0]\n",
        "\n",
        "    #dimension of the space\n",
        "    self.feature_size = x.shape[1]\n",
        "\n",
        "    self.k = k\n",
        "    self.max_iters = max_iters\n",
        "\n",
        "    self.centers = np.zeros((k, x.shape[1]))\n",
        "\n",
        "    #cluster assignment of the datapoints\n",
        "    self.predictions = np.zeros(self.data_size)\n",
        "\n",
        "  def Kmeans_plus_plus(self):\n",
        "  ###############################################################################\n",
        "    ## Insert your code here to:\n",
        "    #   * Initialize k centers using the Kmeans++ algorithm\n",
        "    #   * Comment the line below to test out your function\n",
        "    raise NotImplementedError(\"Please complete This part\")\n",
        "    ###############################################################################\n",
        "    centers = np.zeros((self.k, self.feature_size))\n",
        "\n",
        "    centers[0] = ...\n",
        "\n",
        "    for i in range(1, self.k):\n",
        "      current_centers = centers[: i]\n",
        "      data_center_distances = find_distance(self.x, current_centers)\n",
        "      \n",
        "      #the minimum distance of datapoints to each of the currently chosen centers\n",
        "      minimum_distances = ...\n",
        "      \n",
        "      #choose the next cluster center\n",
        "      centers[i] = ...\n",
        "\n",
        "    return centers\n",
        "\n",
        "  def random_init(self):\n",
        "  ###############################################################################\n",
        "    ## Insert your code here to:\n",
        "    #   * Initialize k centers using the random gaussian initialization\n",
        "    #   * Comment the line below to test out your function\n",
        "    raise NotImplementedError(\"Please complete This part\")\n",
        "    ###############################################################################\n",
        "\n",
        "    #mean of the datapoints\n",
        "    mean = ...\n",
        "\n",
        "    #standard deviation of the datapoints\n",
        "    std = ...\n",
        "\n",
        "    #choose k centers with a gaussian random number generator\n",
        "    centers = ...\n",
        "\n",
        "    return centers\n",
        "\n",
        "  def predict(self, init_mode = 'random'):\n",
        "    ##################################Inputs##################################\n",
        "    # init_mode: The initialization method. can be either 'random' or 'kmeans++'\n",
        "    ##########################################################################\n",
        "\n",
        "    \n",
        "    ##############################################################################\n",
        "    ## Insert your code here to:\n",
        "    #   * Initialize the cluster centers\n",
        "    #   * Run the two step algorithm of Kmeans for \"max_iter iterations\"\n",
        "    raise NotImplementedError(\"Please complete This part\")\n",
        "    ###############################################################################\n",
        "\n",
        "    self.centers = ...\n",
        "\n",
        "    for i in range(self.max_iters):\n",
        "      data_center_dists = find_distance(self.x, self.centers)\n",
        "      #step 1 of the algorithm: assign each datapoint to the cluster which has the closest center.\n",
        "      self.predictions = ...\n",
        "\n",
        "      prev_centers = np.copy(self.centers)\n",
        "      \n",
        "      #step 2 of the algorithm: calculate cluster centers using the new assignments\n",
        "\n",
        "      for j in range(self.k):\n",
        "        # datapoints in cluster j\n",
        "        cluster_j = ...\n",
        "\n",
        "        if len(cluster_j) > 0:\n",
        "          #calculate cluster js center\n",
        "          self.centers[j] = ...\n",
        "      \n",
        "      # if centers haven't changed, abort\n",
        "      if (prev_centers == self.centers).all():\n",
        "        return\n",
        "\n",
        "  def calc_distortion(self):\n",
        "    \"\"\" please do not edit this function \"\"\"\n",
        "    distortion = 0\n",
        "\n",
        "    for i in range(self.k):\n",
        "      cluster_x = self.x[self.predictions == i]\n",
        "      cluster_center = self.centers[i]\n",
        "      if len(cluster_x) > 0:\n",
        "        cluster_distances = find_distance(cluster_x, np.expand_dims(cluster_center, 0))\n",
        "\n",
        "        distortion += cluster_distances.sum()\n",
        "    \n",
        "    return distortion / self.data_size\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6Ewk4g9cjEt",
        "cellView": "form"
      },
      "source": [
        "#@title click to view solution or run the cell\n",
        "class Kmeans:\n",
        "\n",
        "  def __init__(self, x, k, max_iters = 1000):\n",
        "    self.x = x\n",
        "    self.data_size = x.shape[0]\n",
        "    self.feature_size = x.shape[1]\n",
        "    self.k = k\n",
        "    self.max_iters = max_iters\n",
        "\n",
        "    self.centers = np.zeros((k, x.shape[1]))\n",
        "  \n",
        "    self.predictions = np.zeros(self.data_size)\n",
        "\n",
        "\n",
        "\n",
        "  def Kmeans_plus_plus(self):\n",
        "    centers = np.zeros((self.k, self.feature_size))\n",
        "  \n",
        "    centers[0] = random.choices(self.x)[0]\n",
        "\n",
        "    for i in range(1, self.k):\n",
        "      current_centers = centers[: i]\n",
        "      data_center_dists = find_distance(self.x, current_centers)\n",
        "\n",
        "      min_dists = data_center_dists.min(axis = 1)\n",
        "      centers[i] = random.choices(self.x, weights = min_dists)[0]\n",
        "    return centers\n",
        "\n",
        "  def random_init(self):\n",
        "      mean = np.mean(self.x, axis = 0)\n",
        "      std = np.std(self.x, axis = 0)\n",
        "\n",
        "      centers = np.random.randn(self.k, self.feature_size) * mean + std\n",
        "\n",
        "      return centers\n",
        "\n",
        "\n",
        "  def predict(self, init_mode = 'random'):\n",
        "\n",
        "    if init_mode == 'random':\n",
        "      self.centers = self.random_init()\n",
        "    elif init_mode == 'kmeans++':\n",
        "      self.centers = self.Kmeans_plus_plus()\n",
        "    for i in range(self.max_iters):\n",
        "      data_center_dists = find_distance(self.x, self.centers)\n",
        "      self.predictions = data_center_dists.argmin(axis = 1)\n",
        "\n",
        "      prev_centers = np.copy(self.centers)\n",
        "\n",
        "      for j in range(self.k):\n",
        "        cluster_j = self.x[self.predictions == j] \n",
        "        if len(cluster_j) > 0:\n",
        "          self.centers[j] = cluster_j.mean(axis = 0)\n",
        "      \n",
        "      if (prev_centers == self.centers).all():\n",
        "        return\n",
        "\n",
        "\n",
        "  def calc_distortion(self):\n",
        "    distortion = 0\n",
        "\n",
        "    for i in range(self.k):\n",
        "      cluster_x = self.x[self.predictions == i]\n",
        "      cluster_center = self.centers[i]\n",
        "      if len(cluster_x) > 0:\n",
        "        cluster_distances = find_distance(cluster_x, np.expand_dims(cluster_center, 0))\n",
        "\n",
        "        distortion += cluster_distances.sum()\n",
        "    \n",
        "    return distortion / self.data_size\n",
        "\n",
        "\n",
        "      \n",
        "    \n",
        "        \n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "      \n",
        "    \n",
        "\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIYIW7xUMArT"
      },
      "source": [
        "You can now check your implementation, by clustering the data we generated before. You can run the algorithm with both of the initialization methods and see how the result changes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SYXga9dqkknB"
      },
      "source": [
        "model = Kmeans(data, 3)\n",
        "model.predict('kmeans++')\n",
        "\n",
        "plt.scatter(data[model.predictions == 0, 0], data[model.predictions == 0, 1])\n",
        "plt.scatter(data[model.predictions == 1, 0], data[model.predictions == 1, 1])\n",
        "plt.scatter(data[model.predictions == 2, 0], data[model.predictions == 2, 1])\n",
        "\n",
        "plt.legend(['cluster 1', 'cluster 2', 'cluster 3'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lShqwu5KNCBX"
      },
      "source": [
        "# How to choose K\n",
        "\n",
        "choosing the number of clusters is a very important part of the Kmeans algorithm and clustering in general. \n",
        "\n",
        "We want to choose a k which yields the best clustering. But first we have to define what is good and what is bad in a clustering.\n",
        "There are some method that measure the __goodness__ of a clustering, which we will discuss here.\n",
        "\n",
        "1. The silhouette method:\n",
        "\n",
        "In this method, for each datapoint $d$ we calculate a score called the silhouette score:\n",
        "\n",
        "$$s(d) = \\frac{b(d) - a(d)}{max(b(d), a(d))}$$\n",
        "\n",
        "In the above equation, $a(d)$ is equal to the average distance of datapoint $d$ to the datapoints in the same cluster which $d$ is in; and $b(d)$ is equal to minimum of the average distances of $d$ to datapoints in each of the clusters which $d$ is not in.\n",
        "\n",
        "You can easily observe that the silhouette score is between -1 and 1. 1 shows that $d$ is compact within its cluster and far away from the other clusters which is good. and -1 shows that $d$ is actually close to the datapoints from other clusters and far from datapoints in the same cluster.\n",
        "\n",
        "averaging the silhouette score, gives us a measure of how good a clustering is and using that, we can determine the best k.\n",
        "\n",
        "2. The Distortion measure:\n",
        "\n",
        "In this method we calculate a measure called __distortion__ to determine how good a clustering is. The distortion measure is calculated using the following equation:\n",
        "$$D(X, M) = \\sum_k{\\frac{1}{C_k}\\sum_n{r^k_n||m_k - x_n||^2}}$$\n",
        "in which $C_k$ is the number of datapoints in cluster $k$.\n",
        "\n",
        "As you can understand, this is simply the sum of average of distance of datapoints to their corresponding  cluster centers for each cluster. Obviously the lower the distortion measure is, the better the clustering is.\n",
        "\n",
        "We have implemented this method in the Kmeans class in \"calc_distortion\" function.\n",
        "\n",
        "In the next cell by using the slidebar, you can observe that by changing k, what happens to the distortion measure."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZI0Nax5KlIxe",
        "cellView": "form"
      },
      "source": [
        "#@title make sure to run this cell to be able to use the widget\n",
        "from ipywidgets import widgets\n",
        "\n",
        "\n",
        "def refresh(k):\n",
        "  k = int(k)\n",
        "  distortions = np.zeros(k)\n",
        "\n",
        "  for i in range(1, k + 1):\n",
        "    model = Kmeans(data, i, 1000)\n",
        "    model.predict()\n",
        "    distortions[i - 1] = model.calc_distortion()\n",
        "\n",
        "  plt.plot(range(1, k + 1), distortions)\n",
        "\n",
        "  plt.xlabel('k')\n",
        "  plt.ylabel('distortion')\n",
        "  plt.show()\n",
        "\n",
        "style = {'description_width': 'initial'}\n",
        "\n",
        "_ = widgets.interact(refresh, \n",
        "                 k = widgets.IntSlider(value = 2, min = 1, max = 20, steps = 1, description = 'K'), style = style)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSsFh0ujTejo"
      },
      "source": [
        "As you can see, the distortion level, decreases sharply up until a certain k and after that, the value doesn't change very much.\n",
        "\n",
        "Evidently, this usually happens, unless the structure of a data isn't clustered or cannot be clustered by the Kmeans algorithm. So we can choose the value of k right before the point the decrease stops. This method is called the __Elbo__ method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHxDENq3UbjS"
      },
      "source": [
        "# Segmentation using Kmeans\n",
        "\n",
        "Although there are much better methods to do image segmentation, Kmeans is a simple way of dividing an image to multiple segments.\n",
        "\n",
        "In the remaining cells, we try to use our implementation to perform segmentation on an image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRE0zA4BWYvA",
        "cellView": "form"
      },
      "source": [
        "#@title Run this cell to load image\n",
        "import cv2 as cv\n",
        "import urllib\n",
        "\n",
        "req = urllib.request.urlopen('https://wehco.media.clients.ellingtoncms.com/img/photos/2019/02/08/Frida_lead_image_t800.png?90232451fbcadccc64a17de7521d859a8f88077d')\n",
        "arr = np.asarray(bytearray(req.read()), dtype=np.uint8)\n",
        "image = cv.imdecode(arr, -1)\n",
        "image = cv.cvtColor(image, cv.COLOR_BGR2RGB)\n",
        "image_flat = image.reshape(-1, 3)\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOaMGAG65HCJ",
        "cellView": "form"
      },
      "source": [
        "#@title make sure to run this cell to be able to use the widget\n",
        "def refresh(k):\n",
        "  \n",
        "  image_flat = image.reshape(-1, 3)\n",
        "  model = Kmeans(image_flat, k, 1000)\n",
        "  model.predict('kmeans++')\n",
        "\n",
        "\n",
        "  image_clustered = model.predictions.reshape((image.shape[0], image.shape[1]))\n",
        "  print('clusters: ', np.unique(image_clustered, return_counts = True)[0])\n",
        "  print('number of points: ', np.unique(image_clustered, return_counts = True)[1])\n",
        "  plt.imshow(image_clustered)\n",
        "  plt.axis('off')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "style = {'description_width': 'initial'}\n",
        "\n",
        "_ = widgets.interact(refresh, \n",
        "                 k = widgets.IntSlider(value = 2, min = 2, max = 50, steps = 1, description = 'K'), style = style)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}